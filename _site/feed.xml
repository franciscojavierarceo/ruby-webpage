<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>francisco javier arceo</title>
    <description></description>
    <link>http://franciscojavierarceo.github.io/</link>
    <atom:link href="http://franciscojavierarceo.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 23 Jul 2016 23:35:01 -0400</pubDate>
    <lastBuildDate>Sat, 23 Jul 2016 23:35:01 -0400</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>Bayesian Deep Learning</title>
        <description>&lt;p&gt;So I’ve been checking out a new Python library, &lt;a href=&quot;https://github.com/blei-lab/edward&quot;&gt;Edward&lt;/a&gt;, developed by &lt;a href=&quot;http://www.cs.columbia.edu/~blei/&quot;&gt;David Blei&lt;/a&gt;’s ML research group at Columbia recently. &lt;/p&gt;

&lt;p&gt;It’s been fairly interesting thusfar, but I haven’t dived extraordinarily deep into the documentation yet. Instead, I spent some time tinkering with one of their examples: &lt;a href=&quot;https://raw.githubusercontent.com/blei-lab/edward/master/examples/bayesian_nn.py&quot;&gt;Bayesian Neural Networks&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;What is this model? It’s fairly interesting. It’s a Neural network with a Gaussian prior distribution on the weights (with an assumed variance of 1 and unknown mean). As I’ve mentioned probably too many times on this site, I’ve studied both Bayesian ML and Deep Learning, so I was particularly excited about this so I could put the two together! Interestingly, Bayesian Deep Learning is a &lt;a href=&quot;https://arxiv.org/pdf/1604.01662v2.pdf&quot;&gt;growing area of research&lt;/a&gt; with a lot of potential. Hopefully, I’ll explore this area more because it’d neatly tie two things I really love. &lt;/p&gt;

&lt;p&gt;Back to my main point in writing this: what I did with the tutorial. &lt;/p&gt;

&lt;p&gt;Their example shows a neat graph of the learned posterior distribution of the means of the weights of a 2-hidden layer neural network at iteration 0 and iteration 1000, which is really cool. I thought it’d be cool to add two things: (1) a look at the Variational objective function and (2) a dynamic, full view of the posterior as a function of the iterations. It’s always interesting to see how parameters behave as ML models begin to learn the appropriate functions, so I thought this exercise would be fun (I also love the excuse to make a gif whenever I can).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/vof_bnn.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above is a look at the variational objective function, it sucks at first (to be excpected) but seems to converge pretty quickly!&lt;/p&gt;

&lt;p&gt;Now here’s a look at the posterior means of each learned neuron/weight. Pretty neat huh? 
&lt;img src=&quot;/assets/images/movie_bayesian_nn.gif&quot; alt=&quot;Whoaaa a gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s all for now, next I want to try this model out on MNIST and see how it performs, which I think will be a fun exercise.&lt;/p&gt;

&lt;p&gt;Anyways, thanks for reading! Laslty, the code is below and on my &lt;a href=&quot;https://github.com/franciscojavierarceo/edward/blob/master/examples/bayesian_nn.py&quot;&gt;github&lt;/a&gt;!&lt;/p&gt;

&lt;h1 id=&quot;code&quot;&gt;Code&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
&quot;&quot;&quot;
Bayesian neural network using mean-field variational inference.
(see, e.g., Blundell et al. (2015); Kucukelbir et al. (2016))
Inspired by autograd&#39;s Bayesian neural network example.

Probability model:
    Bayesian neural network
    Prior: Normal
    Likelihood: Normal with mean parameterized by fully connected NN
Variational model
    Likelihood: Mean-field Normal
&quot;&quot;&quot;
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import imageio
import edward as ed
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

from edward.models import Variational, Normal
from edward.stats import norm
from edward.util import rbf

def create_dir(art_id):
    &quot;&quot;&quot;
    Creates a new directory for the article if it doesn&#39;t exist
    :param art_id: string, descriptive folder name for article

    :returns out_path: abs path to new directory
    &quot;&quot;&quot;
    # I suspect this is unnecessary because sys.argv will error
    if &#39; &#39; in art_id:
        art_id = art_id.replace(&#39; &#39;, &#39;_&#39;)

    if not os.path.isdir(art_id):
        os.mkdir(art_id)

    return os.path.join(os.getcwd(), art_id)

class BayesianNN:
    &quot;&quot;&quot;
    Bayesian neural network for regressing outputs y on inputs x.

    p((x,y), z) = Normal(y | NN(x; z), lik_variance) *
                  Normal(z | 0, prior_variance),

    where z are neural network weights, and with known lik_variance
    and prior_variance.

    Parameters
    ----------
    layer_sizes : list
        The size of each layer, ordered from input to output.
    nonlinearity : function, optional
        Non-linearity after each linear transformation in the neural
        network; aka activation function.
    lik_variance : float, optional
        Variance of the normal likelihood; aka noise parameter,
        homoscedastic variance, scale parameter.
    prior_variance : float, optional
        Variance of the normal prior on weights; aka L2
        regularization parameter, ridge penalty, scale parameter.
    &quot;&quot;&quot;
    def __init__(self, layer_sizes, nonlinearity=tf.nn.tanh,
        lik_variance=0.01, prior_variance=1):
        self.layer_sizes = layer_sizes
        self.nonlinearity = nonlinearity
        self.lik_variance = lik_variance
        self.prior_variance = prior_variance

        self.n_layers = len(layer_sizes)
        self.weight_dims = list(zip(layer_sizes[:-1], layer_sizes[1:]))
        self.n_vars = sum((m+1)*n for m, n in self.weight_dims)

    def unpack_weights(self, z):
        &quot;&quot;&quot;Unpack weight matrices and biases from a flattened vector.&quot;&quot;&quot;
        for m, n in self.weight_dims:
            yield tf.reshape(z[:m*n],        [m, n]), \
                  tf.reshape(z[m*n:(m*n+n)], [1, n])
            z = z[(m+1)*n:]

    def neural_network(self, x, zs):
        &quot;&quot;&quot;
        Return a `n_samples` x `n_minibatch` matrix. Each row is
        the output of a neural network on the input data `x` and
        given a set of weights `z` in `zs`.
        &quot;&quot;&quot;
        matrix = []
        for z in tf.unpack(zs):
            # Calculate neural network with weights given by `z`.
            h = x
            for W, b in self.unpack_weights(z):
                # broadcasting to do (h*W) + b (e.g. 40x10 + 1x10)
                h = self.nonlinearity(tf.matmul(h, W) + b)

            matrix += [tf.squeeze(h)] # n_minibatch x 1 to n_minibatch

        return tf.pack(matrix)

    def log_prob(self, xs, zs):
        &quot;&quot;&quot;Return a vector [log p(xs, zs[1,:]), ..., log p(xs, zs[S,:])].&quot;&quot;&quot;
        x, y = xs[&#39;x&#39;], xs[&#39;y&#39;]
        log_prior = -tf.reduce_sum(zs*zs, 1) / self.prior_variance
        mus = self.neural_network(x, zs)
        # broadcasting to do mus - y (n_samples x n_minibatch - n_minibatch)
        log_lik = -tf.reduce_sum(tf.pow(mus - y, 2), 1) / self.lik_variance
        return log_lik + log_prior

def build_toy_dataset(N=40, noise_std=0.1):
    ed.set_seed(0)
    D = 1
    x  = np.concatenate([np.linspace(0, 2, num=N/2),
                         np.linspace(6, 8, num=N/2)])
    y = np.cos(x) + norm.rvs(0, noise_std, size=N)
    x = (x - 4.0) / 4.0
    x = x.reshape((N, D))
    return {&#39;x&#39;: x, &#39;y&#39;: y}

ed.set_seed(42)
# model = BayesianNN(layer_sizes=[1, 10, 10, 1], nonlinearity=rbf)
model = BayesianNN(layer_sizes=[1, 10, 10, 1], nonlinearity= tf.nn.tanh)
variational = Variational()
variational.add(Normal(model.n_vars))
data = build_toy_dataset()

sess = ed.get_session()
inference = ed.MFVI(model, variational, data)
inference.initialize(n_print=1)
loss_vof = []

create_dir(&#39;./tmp_plots&#39;)

for t in range(1000):
    loss = inference.update()
    loss_vof.append(loss)
    if t % inference.n_print == 0:
        # print(&quot;iter {:d} loss {:.2f}&quot;.format(t, loss))
        # Sample functions from variational model
        mean, std = sess.run([variational.layers[0].loc,
                              variational.layers[0].scale])
        rs = np.random.RandomState(0)
        zs = rs.randn(10, variational.n_vars) * std + mean
        zs = tf.constant(zs, dtype=tf.float32)
        inputs = np.linspace(-8, 8, num=400, dtype=np.float32)
        x = tf.expand_dims(tf.constant(inputs), 1)
        mus = model.neural_network(x, zs)
        outputs = mus.eval()

        # Get data
        x, y = data[&#39;x&#39;], data[&#39;y&#39;]
        # Plot data and functions
        fig = plt.figure(figsize=(8,8), facecolor=&#39;white&#39;)
        ax = fig.add_subplot(111, frameon=False)
        ax.plot(x, y, &#39;bx&#39;)
        ax.plot(inputs, outputs.T)
        ax.set_xlim([-10, 10])
        ax.set_ylim([-2, 3])
        # Adding leading zeros for the sort later

        t = str(t)
        if len(t)!=3:
            t = ((3-len(t)) * &#39;0&#39;) + t

        plt.savefig(&#39;./tmp_plots/%i_p.jpeg&#39; % t)
        plt.close()

# Pulling in the images that were exported 
file_names = sorted((fn for fn in os.listdir(&#39;./tmp_plots/&#39;) if fn.endswith(&#39;_p.jpeg&#39;)))

# Collecting all of the images
images = []
for filename in file_names:
    images.append(imageio.imread(&#39;./tmp_plots/&#39;+filename))
imageio.mimsave(&#39;movie_bayesian_nn.gif&#39;, images)

# Removing all of the old files
#os.system(&#39;rm -rf ./tmp_plots/&#39;)

# Plotting the variational objective function
plt.figure(figsize=(12,8))
plt.plot(range(len(loss_vof)), loss_vof, c=&#39;red&#39;)
plt.grid() 
plt.title(&#39;Variational Objective Function&#39;)
plt.ylabel(&quot;Variational Objective Function&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.savefig(&#39;vof_bnn.jpeg&#39;)
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Sat, 23 Jul 2016 00:00:00 -0400</pubDate>
        <link>http://franciscojavierarceo.github.io/2016/07/23/Bayesian-Deep-Learning</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2016/07/23/Bayesian-Deep-Learning</guid>
      </item>
    
      <item>
        <title>2016 has been crazy</title>
        <description>&lt;p&gt;It’s been a hectic year thusfar, I spent most of my time either at work, on the train, in class, at the gym, or with my lovely girlfirend Stephany (probably the best part of my year). &lt;/p&gt;

&lt;p&gt;During this chaotic year I got the opportunity to improve my computer science (CS) chops (e.g., building my website), learn a ton more of (&lt;a href=&quot;https://github.com/franciscojavierarceo/EECS6892&quot;&gt;Bayesian&lt;/a&gt; &lt;a href=&quot;https://github.com/franciscojavierarceo/COMS4721&quot;&gt;Machine Learning&lt;/a&gt;, and dive into &lt;a href=&quot;https://github.com/franciscojavierarceo/ECBME6040&quot;&gt;Deep Learning&lt;/a&gt;. At the gym I was able to also increase my deadlift from a pathetic 185 to 405 (still not great)! And increase my bench from 155 to 215!! So to say the least it’s been a pretty awesome year, for both nerdy and non-nerdy reasons.&lt;/p&gt;

&lt;p&gt;This summer I decided to take a Mathematical Analysis course using the classic Rudin textbook and boy has that been an experience. It’s been my first pure mathematics course in nearly 9 years, so to say that it’s been a challenge is an understatement but I &lt;em&gt;love&lt;/em&gt; the material. It’s absolutely fascinating to me, regardless of how much work I have to put in to actually understand it. &lt;/p&gt;

&lt;p&gt;Also, let me just say, Stephany is amazing. During those late nights of studying and homework, she was sweet enough to feed me, keep me company, and keep me sane. I am so grateful to have such a caring, beautiful, and loving woman in my life. :) &lt;/p&gt;

&lt;p&gt;Professionally, my work/career has also been going well, I love my job and my team is really incredible. My boss is the best, he’s been encouraging me to learn a ton of new stuff and I feel more and more like a real computer scientist (but I have a long way to go before I actually call myself that). The learning curve has also been steep, but I feel like I’m making progress, which is really rewarding. I love computers and I love CS, so I really want to explore it more…maybe I’ll pursue a third masters in CS or EECS (Electrical Engineering and CS) and finally get into hardware.&lt;/p&gt;

&lt;p&gt;That might be something I explore in the future. I love school, learning about new things, and getting a degree out of the whole thing, so maybe I will pursue a third masters, I guess we’ll see what 2017 holds.&lt;/p&gt;

&lt;p&gt;Anyways, thanks for reading! I just wanted to take a second to review how simultaneously exhausting and wonderful this year has been. Going to school and working a full-time job is a handful, but I guess old habits die hard. &lt;/p&gt;
</description>
        <pubDate>Wed, 20 Jul 2016 00:00:00 -0400</pubDate>
        <link>http://franciscojavierarceo.github.io/2016/07/20/Hectic-Year</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2016/07/20/Hectic-Year</guid>
      </item>
    
      <item>
        <title>My first post</title>
        <description>&lt;h1 id=&quot;jekyll-pages-python-and-learning-to-work-with-github&quot;&gt;Jekyll pages, Python, and learning to work with GitHub&lt;/h1&gt;

&lt;p&gt;I recently began working with GitHub and learning the ways of the codes. &lt;em&gt;It’s amazing.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;GitHub is a great way to version your code, especially when working with groups, to ensure 
recovery and quality. GitHub Markdown (formatted) pages are really great because of the ease and intuitive behavior.&lt;/p&gt;

&lt;h2 id=&quot;things-i-hope-to-accomplish-in-this-blog&quot;&gt;Things I hope to accomplish in this blog:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Write about Machine Learning topics that I find interesting&lt;/li&gt;
  &lt;li&gt;Provide an “economist’s” introduction to hacking/data science&lt;/li&gt;
  &lt;li&gt;Provide more clear insight on what some of the fancy machine learning algorithms do.&lt;/li&gt;
  &lt;li&gt;Provide an overview of some simple NLP methods&lt;/li&gt;
  &lt;li&gt;Give my personal opinions about various tools/pieces of software&lt;/li&gt;
  &lt;li&gt;Create cool visualizations of data
    &lt;ul&gt;
      &lt;li&gt;Maybe I’ll even offer a tutorial&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;## Code
One of the many great things that Jekyll does is formatting code for you. Below is some simple code
that takes in a column of text (from some arbitrary .csv file) and outputs a sparse matrix
of unigrams/bag-of-words stored as binary variables (also called one-hot-encoding)
that you can use for a simple regression (which is what this code does).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import scipy
import os, sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model, decomposition
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
def roc_plot(actual,pred,ttl):
	fpr, tpr, thresholds = roc_curve(actual, pred)
	roc_auc = auc(fpr, tpr)
    print(&quot;The Area Under the ROC Curve : %f&quot; % roc_auc)
    # Plot ROC curve
	plt.clf()
    plt.plot(fpr, tpr, color=&#39;red&#39;,label=&#39;ROC curve (area = %0.2f)&#39; % roc_auc)
    plt.plot([0, 1], [0, 1], &#39;k&#39;)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
	plt.grid()
	plt.xlabel(&#39;False Positive Rate&#39;)
	plt.ylabel(&#39;True Positive Rate&#39;)
    plt.title(&#39;ROC Curve&#39;+&#39;\n&#39;+ttl)
    plt.legend(loc=&quot;lower right&quot;)
    plt.show()

def Build_STDM(docs, **kwargs):
	&#39;&#39;&#39; Build Sparse Term Document Matrix &#39;&#39;&#39;
	vectorizer = CountVectorizer(**kwargs)
	sparsematrix= vectorizer.fit_transform(docs)
    vocab = vectorizer.vocabulary_.keys()
    return sparsematrix, vocab

df1 = pd.read_csv(&#39;myregressiondata.csv&#39;,sep=&#39;,&#39;)
varchar = df1[&#39;MyTextField&#39;]
xs = xs[:,1:10]
y = np.hstack((np.ones(100/2),np.zeros(100/2))).reshape((100,1))
betas = scipy.sparse.linalg.inv(xs.T.dot(xs)).dot(xs.T).dot(y)
bdf= pd.DataFrame()
bdf[&#39;Words&#39;] = vocab[1:10]
bdf[&#39;betas&#39;] = betas

print bdf
logistic = linear_model.LogisticRegression(penalty=&#39;l2&#39;,tol=0.0001,
                                       fit_intercept=True,intercept_scaling=1)
MyModel = logistic.fit(xs,y)
betas = MyModel.coef_.ravel()
ys = logistic.predict_proba(xs)[:,0]

# Call the function on e-mail messages. The token_pattern is set so that terms are only
# words with two or more letters (no numbers or punctuation)
xs, vocab = Build_STDM(varchar)
print xs[:,1:10]
print vocab[0:10]
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Thu, 12 Nov 2015 00:00:00 -0500</pubDate>
        <link>http://franciscojavierarceo.github.io/2015/11/12/First-Post</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2015/11/12/First-Post</guid>
      </item>
    
  </channel>
</rss>
