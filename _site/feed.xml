<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>francisco javier arceo</title>
    <description></description>
    <link>http://franciscojavierarceo.github.io/</link>
    <atom:link href="http://franciscojavierarceo.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 11 Nov 2015 13:24:23 -0500</pubDate>
    <lastBuildDate>Wed, 11 Nov 2015 13:24:23 -0500</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>My first post</title>
        <description>&lt;h1&gt;Jekyll pages, Python, and learning to work with GitHub&lt;/h1&gt;

&lt;p&gt;I recently began working with GitHub and learning the ways of the codes. &lt;em&gt;It&amp;#39;s amazing.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;GitHub is a great way to version your code, especially when working with groups, to ensure 
recovery and quality. GitHub Markdown (formatted) pages are really great because of the ease and intuitive behavior.&lt;/p&gt;

&lt;h2&gt;Things I hope to accomplish in this blog:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Write about Machine Learning topics that I find interesting&lt;/li&gt;
&lt;li&gt;Provide an &amp;quot;economist&amp;#39;s&amp;quot; introduction to hacking/data science&lt;/li&gt;
&lt;li&gt;Provide more clear insight on what some of the fancy machine learning algorithms do.&lt;/li&gt;
&lt;li&gt;Provide an overview of some simple NLP methods&lt;/li&gt;
&lt;li&gt;Give my personal opinions about various tools/pieces of software&lt;/li&gt;
&lt;li&gt;Create cool visualizations of data

&lt;ul&gt;
&lt;li&gt;Maybe I&amp;#39;ll even offer a tutorial&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;h2&gt;Code&lt;/h2&gt;

&lt;p&gt;One of the many great things that Jekyll does is formatting code for you. Below is some simple code
that takes in a column of text (from some arbitrary .csv file) and outputs a sparse matrix
of unigrams/bag-of-words stored as binary variables (also called one-hot-encoding)
that you can use for a simple regression (which is what this code does).&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;import scipy
import os, sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model, decomposition
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
def roc_plot(actual,pred,ttl):
    fpr, tpr, thresholds = roc_curve(actual, pred)
    roc_auc = auc(fpr, tpr)
    print(&amp;quot;The Area Under the ROC Curve : %f&amp;quot; % roc_auc)
    # Plot ROC curve
    plt.clf()
    plt.plot(fpr, tpr, color=&amp;#39;red&amp;#39;,label=&amp;#39;ROC curve (area = %0.2f)&amp;#39; % roc_auc)
    plt.plot([0, 1], [0, 1], &amp;#39;k&amp;#39;)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.grid()
    plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)
    plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)
    plt.title(&amp;#39;ROC Curve&amp;#39;+&amp;#39;\n&amp;#39;+ttl)
    plt.legend(loc=&amp;quot;lower right&amp;quot;)
    plt.show()

def Build_STDM(docs, **kwargs):
    &amp;#39;&amp;#39;&amp;#39; Build Sparse Term Document Matrix &amp;#39;&amp;#39;&amp;#39;
    vectorizer = CountVectorizer(**kwargs)
    sparsematrix= vectorizer.fit_transform(docs)
    vocab = vectorizer.vocabulary_.keys()
    return sparsematrix, vocab

df1 = pd.read_csv(&amp;#39;myregressiondata.csv&amp;#39;,sep=&amp;#39;,&amp;#39;)
varchar = df1[&amp;#39;MyTextField&amp;#39;]
xs = xs[:,1:10]
y = np.hstack((np.ones(100/2),np.zeros(100/2))).reshape((100,1))
betas = scipy.sparse.linalg.inv(xs.T.dot(xs)).dot(xs.T).dot(y)
bdf= pd.DataFrame()
bdf[&amp;#39;Words&amp;#39;] = vocab[1:10]
bdf[&amp;#39;betas&amp;#39;] = betas

print bdf
logistic = linear_model.LogisticRegression(penalty=&amp;#39;l2&amp;#39;,tol=0.0001,
                                       fit_intercept=True,intercept_scaling=1)
MyModel = logistic.fit(xs,y)
betas = MyModel.coef_.ravel()
ys = logistic.predict_proba(xs)[:,0]

# Call the function on e-mail messages. The token_pattern is set so that terms are only
# words with two or more letters (no numbers or punctuation)
xs, vocab = Build_STDM(varchar)
print xs[:,1:10]
print vocab[0:10]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        <pubDate>Thu, 05 Mar 2015 00:00:00 -0500</pubDate>
        <link>http://franciscojavierarceo.github.io/2015/03/05/First-Post</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2015/03/05/First-Post</guid>
      </item>
    
  </channel>
</rss>
