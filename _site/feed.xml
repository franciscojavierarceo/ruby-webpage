<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>francisco javier arceo</title>
    <description></description>
    <link>http://franciscojavierarceo.github.io/</link>
    <atom:link href="http://franciscojavierarceo.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 03 Aug 2016 20:55:04 -0400</pubDate>
    <lastBuildDate>Wed, 03 Aug 2016 20:55:04 -0400</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>GBMs, Step Functions, and Deep Learning</title>
        <description>&lt;p&gt;Deep Learning is awesome, I’m a huge fan. I took a class in the Spring of 2016 just on &lt;a href=&quot;https://github.com/franciscojavierarceo/ECBME6040&quot;&gt;Deep Learning&lt;/a&gt; and got to dive into the really awesome work people are doing in computer vision, NLP, and data mining.&lt;/p&gt;

&lt;p&gt;I’ve recently been trying to test DL on more general problems and have found that DL algorithms (e.g., MLPs) don’t necessarily do better than other algorithms all the time, which is not shocking. In fact, I’ve heard passively in class that a proof exists showing that for any given algorithm, you can always construct a dataset it will fail to learn. In the DL literature we call data like this &lt;a href=&quot;https://arxiv.org/pdf/1412.6572.pdf&quot;&gt;adversarial examples&lt;/a&gt;. So I started to look a little more into this problem and I thought it would be fun to make a quick &lt;a href=&quot;https://github.com/franciscojavierarceo/Python/blob/master/Step%20Functions.ipynb&quot;&gt;simulation&lt;/a&gt; and blog post about my findings.  &lt;/p&gt;

&lt;p&gt;So, one thing that’s great about decision trees is that they can learn very jagged functions, (e.g., 2-dimensional step-functions). So I simulated some data with a fairly silly output function that looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/stepfunction_true.png&quot; alt=&quot;A look at the true function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here at the critical points of the input, the output simply shifts by a constant value. This is a pretty ugly function because it almost doesn’t seem continuous but it is.&lt;/p&gt;

&lt;p&gt;Now we can estimate this function using Sklearn’s &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html&quot;&gt;Gradient Boosting Machine&lt;/a&gt; in Python prettty trivially, then we can look at what the model thinks the test set should be given the input data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/stepfunction_gbm.png&quot; alt=&quot;Gradient Boosting Machine&#39;s Estimation of the Function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see that the GBM does a great job! In fact, we can calculate the RMSE here and find that it’s almost 0. How does a Neural Network do? I used an MLP with 10 hidden layers with 200 neurons/units in each layer and ReLU activation units. Here’s what the NN learned:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/stepfunction_nns.png&quot; alt=&quot;Nueral Network&#39;s Estimation of the Function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interesting right? The NN learns a smooth function and seems to struggle with the boundary points of the threshold function, I tried experimenting with a variety of different parameters but most provided a result similar to this one. &lt;/p&gt;

&lt;p&gt;Here we find that the specification for this NN wasn’t able to learn this threshold function but we were able to learn it trivially using  GBM with respect to computation time of both algorithms. This most certaintly has to do with the activation functions and the fact that NN tries to learn a smooth decision boundary near the critical points.&lt;/p&gt;

&lt;p&gt;Lastly, I thought I’d juxtapose the two residual plots (plots of the errors) as function of the two features because it further emphasizes the point the the MLP is learning an overly smooth of a function.&lt;/p&gt;

&lt;center&gt; &lt;img src=&quot;/assets/images/stepfunction_res.png&quot; width=&quot;1400&quot; /&gt; &lt;/center&gt;

&lt;p&gt;Neural Networks and Deep Learning are awesome and have a huge amount of potential in the future and have had a ton of success in the present, I think it’d be interesting to see what modifications to neural networks are necessary to make learning step functions of this nature easier for them to learn.&lt;/p&gt;

&lt;p&gt;Anyways, thanks for reading!
-Francisco&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Aug 2016 00:00:00 -0400</pubDate>
        <link>http://franciscojavierarceo.github.io/2016/08/01/DL-GBM-Step-Functions</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2016/08/01/DL-GBM-Step-Functions</guid>
      </item>
    
      <item>
        <title>PCA and Regression</title>
        <description>&lt;p&gt;So one thing I’ve always found myself puzzled by is why someone would use PCA as a first-stage model and feeding the scores into a second stage model (e.g., linear regression). Doing this (potentially) loses a lot information, so I don’t see why people think it makes sense. &lt;/p&gt;

&lt;p&gt;I understand the argument about stabalizing the variance of your input space, but with sufficient data and a good regularization strategy, you can solve those problems directly. &lt;/p&gt;

&lt;p&gt;So I decided to make a &lt;a href=&quot;https://github.com/franciscojavierarceo/Python/blob/master/SVD%20and%20Regression.ipynb&quot;&gt;Jupyter Notebook&lt;/a&gt; highlighting why it’s a bad choice. The intuition is identical to what I said above, but with a little more mathematical rigor and some simulations to provide some additional validation. &lt;/p&gt;

&lt;p&gt;The neat idea I had was to visualize how the error of the PCA approximation and the linear regression error behave as a function of (1) the true error, (2) the rank of the approximation in PCA, (3) the true rank of the original matrix, and (4) the correlation structure of the features/design matrix. This simulation samples from a multivariate normal and assumes a normally distributed error term (it may or may not be interesting to see how this behaves for discrete distrbutions–though I expect it’d be similar).&lt;/p&gt;

&lt;p&gt;So here’s what happened in the first simulation I specificed an uncorrelated covariance matrix (features are not correlated) and I chose a matrix rank from 10 - 31 and an approximation of 1-K components with an error iterated from 1 to 3. Below is the gif as with the error as the time dimension.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3dplot_gif.GIF&quot; alt=&quot;Whoaaa a 3d gif with a gradient&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pretty neat visualization, right? The z axis shows the ratio of two &lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;&gt;RMSEs&lt;/a&gt; one when using all of the features for the linear regression and the second when using the SVD/PCA scores as a feed into a second stage model.&lt;/p&gt;

&lt;p&gt;We see that when we have large error the approximation is less impactful, but as we decrease the amount of noise in the system the performance the low-rank approximation causes us to do much worse, which is intuitive because as the noise decreases in the system the features are proportionally more important, so taking an approximation of them and attributing the error to your outcome becomes more impactful, so our ability to learn the function reduces.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3dplotcorr_gif.GIF&quot; alt=&quot;Even more gif, yay&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the first simulation I generated the covariance matrix from a uniform distribution but this implies the features are uncorrelated, so I thought I’d experiment with a correlated covariance matrix. Here we see that the the ratio of the two models now produces a varying shape on the z axis, which makes sense since different rank approximations will produce different shapes for varying true ranks, but ultimately the conclusion remains the same, using PCA/SVD as a pipeline to a regression model leaves to a significant degridation in model performance.&lt;/p&gt;

&lt;p&gt;Obviously this particular example was pathological and we &lt;em&gt;defined&lt;/em&gt; the outcome to be exactly our reconstruction error, we can obviously create an example that loses no performance in model degredation but it’s very rare that we will do better from PCA/SVD and if we do, it’s only likely to be due to sampling error and is unlikely to hold repeatedly. &lt;/p&gt;

&lt;p&gt;Again, PCA/SVD is awesome for a ton of other things, but it’s just inappropriate to use as a pipeline for others. Use the features themselves, you can always do better.&lt;/p&gt;

</description>
        <pubDate>Fri, 29 Jul 2016 00:00:00 -0400</pubDate>
        <link>http://franciscojavierarceo.github.io/2016/07/29/PCA-Regression</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2016/07/29/PCA-Regression</guid>
      </item>
    
      <item>
        <title>Bayesian Deep Learning: Mnist</title>
        <description>
</description>
        <pubDate>Tue, 26 Jul 2016 00:00:00 -0400</pubDate>
        <link>http://franciscojavierarceo.github.io/2016/07/26/Bayesian-Deep-Learning-MNIST</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2016/07/26/Bayesian-Deep-Learning-MNIST</guid>
      </item>
    
      <item>
        <title>Bayesian Deep Learning</title>
        <description>&lt;p&gt;So I’ve been checking out a new Python library, &lt;a href=&quot;https://github.com/blei-lab/edward&quot;&gt;Edward&lt;/a&gt;, developed by &lt;a href=&quot;http://www.cs.columbia.edu/~blei/&quot;&gt;David Blei&lt;/a&gt;’s ML research group at Columbia recently. &lt;/p&gt;

&lt;p&gt;It’s been fairly interesting thusfar, but I haven’t dived extraordinarily deep into the documentation yet. Instead, I spent some time tinkering with one of their examples: &lt;a href=&quot;https://raw.githubusercontent.com/blei-lab/edward/master/examples/bayesian_nn.py&quot;&gt;Bayesian Neural Networks&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;What is this model? It’s fairly interesting. It’s a Neural network with a Gaussian prior distribution on the weights (with an assumed variance of 1 and unknown mean). As I’ve mentioned probably too many times on this site, I’ve studied both Bayesian ML and Deep Learning, so I was particularly excited about this so I could put the two together! Interestingly, Bayesian Deep Learning is a &lt;a href=&quot;https://arxiv.org/pdf/1604.01662v2.pdf&quot;&gt;growing area of research&lt;/a&gt; with a lot of potential. Hopefully, I’ll explore this area more because it’d neatly tie two things I really love. &lt;/p&gt;

&lt;p&gt;Back to my main point in writing this: what I did with the tutorial. &lt;/p&gt;

&lt;p&gt;Their example shows a neat graph of the learned posterior predictive distribution of the of a 2-hidden layer neural network at iteration 0 and iteration 1000, which is really cool. I thought it’d be cool to add two things: (1) a look at the Variational objective function and (2) a dynamic, full view of the posterior as a function of the iterations. It’s always interesting to see how parameters behave as ML models begin to learn the appropriate functions, so I thought this exercise would be fun (I also love the excuse to make a gif whenever I can).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/vof_bnn.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above is a look at the variational objective function, it sucks at first (to be excpected) but seems to converge pretty quickly!&lt;/p&gt;

&lt;p&gt;Now here’s a look at the posterior predictive learned during training. Pretty neat huh? 
&lt;img src=&quot;/assets/images/movie_bayesian_nn.gif&quot; alt=&quot;Whoaaa a gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s all for now, next I want to try this model out on MNIST and see how it performs, which I think will be a fun exercise.&lt;/p&gt;

&lt;p&gt;Anyways, thanks for reading! Laslty, feel free to peak at the code to make this gif. It’s available on my &lt;a href=&quot;https://github.com/franciscojavierarceo/edward/blob/master/examples/bayesian_nn.py&quot;&gt;github&lt;/a&gt;!&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Jul 2016 00:00:00 -0400</pubDate>
        <link>http://franciscojavierarceo.github.io/2016/07/23/Bayesian-Deep-Learning</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2016/07/23/Bayesian-Deep-Learning</guid>
      </item>
    
      <item>
        <title>2016 has been crazy</title>
        <description>&lt;p&gt;It’s been a hectic year thusfar, I spent most of my time either at work, on the train, in class, at the gym, or with my lovely girlfirend Stephany (probably the best part of my year). &lt;/p&gt;

&lt;p&gt;During this chaotic year I got the opportunity to improve my computer science (CS) chops (e.g., building my website), learn a ton more of (&lt;a href=&quot;https://github.com/franciscojavierarceo/EECS6892&quot;&gt;Bayesian&lt;/a&gt; &lt;a href=&quot;https://github.com/franciscojavierarceo/COMS4721&quot;&gt;Machine Learning&lt;/a&gt;, and dive into &lt;a href=&quot;https://github.com/franciscojavierarceo/ECBME6040&quot;&gt;Deep Learning&lt;/a&gt;. At the gym I was able to also increase my deadlift from a pathetic 185 to 405 (still not great)! And increase my bench from 155 to 215!! So to say the least it’s been a pretty awesome year, for both nerdy and non-nerdy reasons.&lt;/p&gt;

&lt;p&gt;This summer I decided to take a Mathematical Analysis course using the classic Rudin textbook and boy has that been an experience. It’s been my first pure mathematics course in nearly 9 years, so to say that it’s been a challenge is an understatement but I &lt;em&gt;love&lt;/em&gt; the material. It’s absolutely fascinating to me, regardless of how much work I have to put in to actually understand it. &lt;/p&gt;

&lt;p&gt;Also, let me just say, Stephany is amazing. During those late nights of studying and homework, she was sweet enough to feed me, keep me company, and keep me sane. I am so grateful to have such a caring, beautiful, and loving woman in my life. :) &lt;/p&gt;

&lt;p&gt;Professionally, my work/career has also been going well, I love my job and my team is really incredible. My boss is the best, he’s been encouraging me to learn a ton of new stuff and I feel more and more like a real computer scientist (but I have a long way to go before I actually call myself that). The learning curve has also been steep, but I feel like I’m making progress, which is really rewarding. I love computers and I love CS, so I really want to explore it more…maybe I’ll pursue a third masters in CS or EECS (Electrical Engineering and CS) and finally get into hardware.&lt;/p&gt;

&lt;p&gt;That might be something I explore in the future. I love school, learning about new things, and getting a degree out of the whole thing, so maybe I will pursue a third masters, I guess we’ll see what 2017 holds.&lt;/p&gt;

&lt;p&gt;Anyways, thanks for reading! I just wanted to take a second to review how simultaneously exhausting and wonderful this year has been. Going to school and working a full-time job is a handful, but I guess old habits die hard. &lt;/p&gt;
</description>
        <pubDate>Wed, 20 Jul 2016 00:00:00 -0400</pubDate>
        <link>http://franciscojavierarceo.github.io/2016/07/20/Hectic-Year</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2016/07/20/Hectic-Year</guid>
      </item>
    
      <item>
        <title>My first post</title>
        <description>&lt;h1 id=&quot;jekyll-pages-python-and-learning-to-work-with-github&quot;&gt;Jekyll pages, Python, and learning to work with GitHub&lt;/h1&gt;

&lt;p&gt;I recently began working with GitHub and learning the ways of the codes. &lt;em&gt;It’s amazing.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;GitHub is a great way to version your code, especially when working with groups, to ensure 
recovery and quality. GitHub Markdown (formatted) pages are really great because of the ease and intuitive behavior.&lt;/p&gt;

&lt;h2 id=&quot;things-i-hope-to-accomplish-in-this-blog&quot;&gt;Things I hope to accomplish in this blog:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Write about Machine Learning topics that I find interesting&lt;/li&gt;
  &lt;li&gt;Provide an “economist’s” introduction to hacking/data science&lt;/li&gt;
  &lt;li&gt;Provide more clear insight on what some of the fancy machine learning algorithms do.&lt;/li&gt;
  &lt;li&gt;Provide an overview of some simple NLP methods&lt;/li&gt;
  &lt;li&gt;Give my personal opinions about various tools/pieces of software&lt;/li&gt;
  &lt;li&gt;Create cool visualizations of data
    &lt;ul&gt;
      &lt;li&gt;Maybe I’ll even offer a tutorial&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;## Code
One of the many great things that Jekyll does is formatting code for you. Below is some simple code
that takes in a column of text (from some arbitrary .csv file) and outputs a sparse matrix
of unigrams/bag-of-words stored as binary variables (also called one-hot-encoding)
that you can use for a simple regression (which is what this code does).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import scipy
import os, sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model, decomposition
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
def roc_plot(actual,pred,ttl):
	fpr, tpr, thresholds = roc_curve(actual, pred)
	roc_auc = auc(fpr, tpr)
    print(&quot;The Area Under the ROC Curve : %f&quot; % roc_auc)
    # Plot ROC curve
	plt.clf()
    plt.plot(fpr, tpr, color=&#39;red&#39;,label=&#39;ROC curve (area = %0.2f)&#39; % roc_auc)
    plt.plot([0, 1], [0, 1], &#39;k&#39;)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
	plt.grid()
	plt.xlabel(&#39;False Positive Rate&#39;)
	plt.ylabel(&#39;True Positive Rate&#39;)
    plt.title(&#39;ROC Curve&#39;+&#39;\n&#39;+ttl)
    plt.legend(loc=&quot;lower right&quot;)
    plt.show()

def Build_STDM(docs, **kwargs):
	&#39;&#39;&#39; Build Sparse Term Document Matrix &#39;&#39;&#39;
	vectorizer = CountVectorizer(**kwargs)
	sparsematrix= vectorizer.fit_transform(docs)
    vocab = vectorizer.vocabulary_.keys()
    return sparsematrix, vocab

df1 = pd.read_csv(&#39;myregressiondata.csv&#39;,sep=&#39;,&#39;)
varchar = df1[&#39;MyTextField&#39;]
xs = xs[:,1:10]
y = np.hstack((np.ones(100/2),np.zeros(100/2))).reshape((100,1))
betas = scipy.sparse.linalg.inv(xs.T.dot(xs)).dot(xs.T).dot(y)
bdf= pd.DataFrame()
bdf[&#39;Words&#39;] = vocab[1:10]
bdf[&#39;betas&#39;] = betas

print bdf
logistic = linear_model.LogisticRegression(penalty=&#39;l2&#39;,tol=0.0001,
                                       fit_intercept=True,intercept_scaling=1)
MyModel = logistic.fit(xs,y)
betas = MyModel.coef_.ravel()
ys = logistic.predict_proba(xs)[:,0]

# Call the function on e-mail messages. The token_pattern is set so that terms are only
# words with two or more letters (no numbers or punctuation)
xs, vocab = Build_STDM(varchar)
print xs[:,1:10]
print vocab[0:10]
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Thu, 12 Nov 2015 00:00:00 -0500</pubDate>
        <link>http://franciscojavierarceo.github.io/2015/11/12/First-Post</link>
        <guid isPermaLink="true">http://franciscojavierarceo.github.io/2015/11/12/First-Post</guid>
      </item>
    
  </channel>
</rss>
