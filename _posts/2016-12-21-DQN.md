---
layout: post
title: 'Deep Q-Networks for Event Summarization'
comments: true
---

I love school and learning is my passion, and while I certainly can learn outside of the classroom, I find myself starry-eyed while on an academic campus.

This semester was my last semester as a master's student in the Data Science department at Columbia, which is extremely bitter sweet. It's nice to complete my second graduate degree, but I certainly will miss studying and learning new material in the classroom environment. 

Either way this past semester has been incredible. I got the chance to spend it doing some real research in Machine Learning and Natural Language Processing with a current Ph.D. student, [Chris Kedzie](http://www.cs.columbia.edu/~kedzie/).

## Deep Q-Networks

So, what'd we build? 

We developed an algorithm to summarize events. One use case is for real-time disaster summarization. The figure below shows what the core algorithm is really doing and how it learns. 

<p style="text-align: center; color:gray;"> 
    <img src="/assets/images/dqn_algorithm.png">
    <br>
    <i>Deep Q-Networks for Event Summarization: Core Algorithm</i>
</p>

And here's a look at how the model is learning to select or skip sentences based on the neural network.


<p style="text-align: center; color:gray;"> 
    <img src="/assets/images/dqn_qlearner.jpg">
    <br>
    <i>Deep Q-Networks for Event Summarization: Neural Network Architecture</i>
</p>

For more technical details you're welcome to look at the [Github Repo](https://github.com/franciscojavierarceo/DQN-Event-Summarization).

Some of the cool components are that we are use (1) Recurrent Neural Networks (i.e., deep learning), (2) Natural Language Processing, and (3) Reinforcement Learning. These are three things I've been really interested in learning so it was a great experience diving more deeply into them. It was also a really cool experience to learn [Lua](https://www.lua.org/) and [Torch](http://torch.ch/). 

I got to present our work earlier this week to the [NLP Group at Columbia](http://www1.cs.columbia.edu/nlp/index.cgi) and it was really great getting to talk to such talented researchers. 

Hopefully Chris and I will be able to polish this work up and submit it for publication in 2017!

Merry Christmas and Happy New Year to all of you!

-Francisco